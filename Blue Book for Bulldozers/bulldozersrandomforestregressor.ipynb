{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Predicting the sale price of bulldozer using ML\nIn this notebook , I am going to go through with the goal of predicting the sale price of Bulldozers.\n\n# 1. Problem definition :\nPredict the sale price of a particular piece of heavy equipment at auction based on it's usage, equipment type, and configuration.\n\n# 2. Data\nThe data is downloaded from the Kaggle \"Blue Book for bulldozer\" competition. https://www.kaggle.com/c/bluebook-for-bulldozers/data\n\nThere are 3 main datasets:\n\nTrain.csv is the training set, which contains data through the end of 2011.\nValid.csv is the validation set, which contains data from January 1, 2012 - April 30, 2012 You make predictions on this set throughout the majority of the competition. Your score on this set is used to create the public Leaderboard.\nTest.csv is the test set, which won't be released until the last week of the competition. It contains data from May 1, 2012 - November 2012. Your score on the test set determines your final rank for the competition.\n# 3. Evaluation\nRMSLE (root mean squared log error) between the actual and predicted auction prices.","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"markdown","source":"# Importing essential tools","metadata":{}},{"cell_type":"code","source":"# Regular EDA and plotting libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# preprocessor\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.impute import SimpleImputer\n\n# Models from Scikit-Learn\nfrom sklearn.ensemble import RandomForestRegressor\n\n# Model Evaluations\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import RandomizedSearchCV, GridSearchCV\nfrom sklearn.metrics import mean_squared_error,mean_squared_log_error,mean_absolute_error,make_scorer\n#Pipeline\nfrom sklearn.pipeline import Pipeline\nplt.style.use('seaborn-whitegrid')\nfrom datetime import datetime","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Load data\nParsing saledate as a Datatime column","metadata":{}},{"cell_type":"code","source":"# combined dataset of training and validation set\ndf = pd.read_csv(\"../input/blue-book-for-bulldozer/Train/Train.csv\",parse_dates=['saledate'],low_memory=False) \n# test set\ntest_df = pd.read_csv(\"../input/blue-book-for-bulldozer/Test.csv\",parse_dates=['saledate'],low_memory=False)\n# sorting df according to the saledate\ndf.sort_values(by='saledate',inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head().T","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.info() # most of the features are having object DataType","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# shape of the dataframe\ndf.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Preprocessing","metadata":{}},{"cell_type":"code","source":"df.isna().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df.isna().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Visualize missing data","metadata":{}},{"cell_type":"code","source":"# visualizing missing entries\ndf_missing_percentage = ((df.isna().sum()/df.shape[0])*100)\ntest_df_missing_percentage = ((df.isna().sum()/df.shape[0])*100)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.DataFrame(df_missing_percentage,columns=['missing%']).sort_values(by='missing%').plot(kind='barh',figsize=(7,15));\nplt.xticks(fontsize = 15);\nplt.yticks(fontsize = 10);","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.DataFrame(test_df_missing_percentage,columns=['missing%']).sort_values(by='missing%').plot(kind='barh',figsize=(7,15));\nplt.xticks(fontsize = 15);\nplt.yticks(fontsize = 10);","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Adding Missing Indicators for Numerical and Categorical columns","metadata":{}},{"cell_type":"code","source":"# First of all, I have concatenated all data points so that we can add missing indicators easily\n# test_df has no SalePrice column , so its data points will have NaN in its SalePrice column when cancatenated with df \nConcat = pd.concat((df,test_df),axis = 0).reset_index(drop=True)\n\n# Converting all columns with object dtype to category dtype\nfor label,content in Concat.items() :\n    if pd.api.types.is_object_dtype(content):\n        Concat[label] = content.astype('category')\n        \n# Enriching features\nConcat['year'] = Concat.saledate.dt.year\nConcat['month']= Concat.saledate.dt.month\nConcat['day']= Concat.saledate.dt.day","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cat=[] # list for storing all columns with 'cstegory' dtype\ncat_missing = [] # list for storing columns with 'category' dtype and having missing values\nnum_missing = [] # list for storing columns with 'numerical' dtype and having missing values","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for label,content in Concat.items():\n    \n    if pd.api.types.is_numeric_dtype(content): # checking for numerical features\n        if content.isna().sum() > 0: # checking if the feature has any missing values\n            Concat[f'{label}_ismissing'] = content.isna()\n            num_missing.append(label)\n            \n    if pd.api.types.is_categorical_dtype(content): # checking for categorical features\n        cat.append(label) \n        if content.isna().sum() > 0: # checking if the feature has any missing values\n            Concat[f'{label}_ismissing'] = content.isna()\n            cat_missing.append(label)\n            \ncat_not_missing = list(set(cat) - set(cat_missing))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Filling categorical values\nOne more reason to make a single dataset of all data points is to cover all possible value category while assigning codes to categorical data.","metadata":{}},{"cell_type":"code","source":"# For missing values in categorical datatype, by default `-1` is assigned for its code, so adding 1 before creating new column\nConcat[cat_missing] = Concat[cat_missing].apply(lambda i : i.cat.codes+1)\n\n# For features with no missing values, simply assigning code\nConcat[cat_not_missing] = Concat[cat_not_missing].apply(lambda i : i.cat.codes)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"(Concat.isna().sum() !=0 ).sum() # out which one is SalePrice , which will not be considered","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Filling numerical values\nFilling the missing values with median\nTo avoid data leakage , we separate training set, validation set and test set","metadata":{}},{"cell_type":"code","source":"train_df = Concat.loc[Concat.saledate.dt.year < 2012, :].drop('saledate', axis=1)\n\nvalid_df = Concat.loc[Concat.saledate <= pd.Timestamp(\n    year=2012, month=4, day=30)].loc[Concat.saledate >= pd.Timestamp(year=2012, month=1, day=1)].drop('saledate', axis=1)\n\ntest_df = Concat.loc[Concat.saledate >=\n                     pd.Timestamp(year=2012, month=4, day=30), :].drop(['SalePrice','saledate'], axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"valid_df.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df[num_missing].isna().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"valid_df[num_missing].isna().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_imputer = SimpleImputer(strategy='median')\ntransformer = ColumnTransformer(transformers=[('num_missing',num_imputer,train_df.columns)],remainder='passthrough',)\n\ntrain_df_filled = transformer.fit_transform(train_df) # fitting on training data \nvalid_df_filled = transformer.transform(valid_df) # transforming test based on training data to avoid data leakage\n\ntrain_df_filled = pd.DataFrame(train_df_filled,columns=train_df.columns)\nvalid_df_filled = pd.DataFrame(valid_df_filled,columns=valid_df.columns)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df_filled","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df_filled[num_missing].isna().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"valid_df_filled[num_missing].isna().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Modelling","metadata":{}},{"cell_type":"code","source":"# separating features and labels\nX_train_filled,y_train_filled = train_df_filled.drop(['SalePrice'],axis=1),train_df_filled.SalePrice \nX_valid_filled,y_valid_filled = valid_df_filled.drop(['SalePrice'],axis=1),valid_df_filled.SalePrice\n\nX_train,y_train = train_df.drop(['SalePrice'],axis=1),train_df_filled.SalePrice\nX_valid,y_valid = valid_df.drop(['SalePrice'],axis=1),valid_df_filled.SalePrice","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}